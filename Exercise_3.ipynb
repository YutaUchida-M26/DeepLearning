{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 \n",
    "\n",
    "---\n",
    "\n",
    "## Part 1 - Create convolutional layer using `numpy` ##\n",
    "\n",
    "Here we will implement forward propagation of one convolutional layer with zero padding using `numpy` library.\n",
    "\n",
    "Convolution functions used in this part of exercise:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**:\n",
    "    \n",
    "- $n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer. If you want to reference a specific layer $l$, you can also write $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ and $n_{C_{prev}}$ denote respectively the height, width and number of channels of the previous layer. If referencing a specific layer $l$, this could also be denoted $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Padding\n",
    "\n",
    "Zero-padding adds zeros around the border of an image. The main benefits of padding are the following:\n",
    "\n",
    "- It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer. \n",
    "\n",
    "- It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.\n",
    "\n",
    "We will [use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html) to pad all the images of a batch of examples X with zeros. Note if you want to pad the array \"a\" of shape $(5,5,5,5,5)$ with `pad = 1` for the 2nd dimension, `pad = 3` for the 4th dimension and `pad = 0` for the rest, you would do:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print(\"x.shape =\", x.shape)\n",
    "print(\"x_pad.shape =\", x_pad.shape)\n",
    "print(\"x[1,1] =\", x[1, 1])\n",
    "print(\"x_pad[1,1] =\", x_pad[1, 1])\n",
    "# Expected output: x.shape = (4, 3, 3, 2)\n",
    "#                  x_pad.shape = (4, 7, 7, 2)\n",
    "#                  x[1,1] = [[ 0.90085595, -0.68372786]...[-0.26788808  0.53035547]]\n",
    "#                  x_pad[1,1] = [[ 0.  0.]...[ 0.  0.]]\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single step of convolution \n",
    "\n",
    "We will apply the filter to a single position of the input. This will be used to build a convolutional unit, which: \n",
    "\n",
    "- Takes an input volume \n",
    "- Applies a filter at every position of the input\n",
    "- Outputs another volume (usually of different size)\n",
    "\n",
    "See lecture slide 23 for detailed explanation.\n",
    "\n",
    "In a computer vision application, each value in the matrix on the left corresponds to a single pixel value, and we convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up and adding a bias. In this first step of the exercise, you will implement a single step of convolution, corresponding to applying a filter to just one of the positions to get a single real-valued output. \n",
    "\n",
    "Later in this notebook, we'll apply this function to multiple positions of the input to implement the full convolutional operation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, K, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "\n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    K -- Kernel parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # Element-wise product between a_slice and K. Do not add the bias yet.\n",
    "    s = a_slice_prev * K\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = float(Z + b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "K = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, K, b)\n",
    "print(\"Z =\", Z)\n",
    "# Expected output: -6.99908945068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks - Forward pass\n",
    "\n",
    "In the forward pass, you will take many filters and convolve them on the input. Each 'convolution' gives you a 2D matrix output. We will then stack these outputs to get a 3D volume, see slides 23-30.\n",
    "\n",
    "The function below is implemented to convolve the filters W on an input activation A_prev. This function takes as input A_prev, the activations output by the previous layer (for a batch of m inputs), F filters/weights denoted by W, and a bias vector denoted by b, where each filter has its own (single) bias. Finally you also have access to the hyperparameters dictionary which contains the stride and the padding. \n",
    "\n",
    "\n",
    "**Reminder**:\n",
    "The formulas relating the output shape of the convolution to the input shape is:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$\n",
    "\n",
    "For this exercise, we won't worry about vectorization, and will just implement everything with for-loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, K, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    K -- Kernel, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from K's shape\n",
    "    (f, f, n_C_prev, n_C) = K.shape\n",
    "\n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "\n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor.\n",
    "    n_H = int((n_H_prev + 2.*pad - f) / stride + 1)\n",
    "    n_W = int((n_W_prev + 2.*pad - f) / stride + 1)\n",
    "\n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = np.zeros([m, n_H, n_W, n_C])\n",
    "\n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    for i in range(m):                                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                      # Select ith training example's padded activation\n",
    "        for h in range(n_H):                            # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                        # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                    # loop over channels (= #filters) of the output volume\n",
    "\n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell).\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Convolve the (3D) slice with the correct filter K and bias b, to get back one output neuron.\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, K[:, :, :, c], b[:, :, :, c])\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, K, b, hparameters)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "K = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\": 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, K, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3, 2, 1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n",
    "# Expectes output: Z's mean = 0.0489952035289\n",
    "#                  Z[3,2,1] = [-0.61490741, -6.7439236, ... 5.18531798, 8.75898442]\n",
    "#                  cache_conv[0][1][2][3] = [-0.20075807, 0.18656139, 0.41005165]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Create Convolutional Neural network using PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define default data type and device for Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)  # we set up a seed so that your output matches ours although the initialization is random.\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define helper functions to load data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('/datasets/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('/datasets/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 6\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print(\"y = \" + str(np.squeeze(train_set_y_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 2, we had built a fully-connected network for similar cat-non-cat dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = np.reshape(train_set_x_orig, (1080, 3, 64, 64))/255.\n",
    "test_set_x = np.reshape(test_set_x_orig, (120, 3, 64, 64))/255.\n",
    "train_set_y = np.reshape(train_set_y_orig, -1).T\n",
    "test_set_y = np.reshape(test_set_y_orig, -1).T\n",
    "\n",
    "print(\"number of training examples = \" + str(train_set_x.shape[0]))\n",
    "print(\"number of test examples = \" + str(test_set_x.shape[0]))\n",
    "print(\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print(\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print(\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will train our model with mini-batches to speed up parameter training process.\n",
    "\n",
    "First, we will create helper function to split our data into batches. This function will be implemented with `numpy`, and we will retrieve these batches later during training. So, PyTorch Tensors for training will be created after obtaining data with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :, :]\n",
    "    shuffled_Y = Y[permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)  # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :, :, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :, :, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it's stated before, we will train our model with mini-batches. So, train Tensors will be created later. But during test time we don't need mini-batches. Then we can create PyTorch Tensors to use it further to test our Convolutional model.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "In previous week assignment and exercise Tensor type of input `x` and output `y` was the same, since binary cross-entropy function expects `FloatTensor` as target output `y`. In this week exercise and assignment we will use cross-entropy cost function, because number of classes in the SIGNS dataset is more than 2. This function expects `LongTensor` as target output `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(test_set_x, dtype=dtype, device=device, requires_grad=False)\n",
    "Y_test = torch.tensor(test_set_y, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "To specify models that are more complex than a sequence of a few Modules, we can define our own Modules by subclassing `nn.Module` and defining a `forward` which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors.\n",
    "\n",
    "This class will contain three layers:\n",
    "1. 2D convolutional layer `Conv2d` with `ReLu` activation function, expects kernel size 3,\n",
    "2. 2D convolutional layer `Conv2d` with `ReLu` activation function, expects kernel size 3,\n",
    "3. `Linear` layer without activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, image_shape, filters, kernels, strides, n_out):\n",
    "        super(CNN, self).__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        image_shape: python list containing shape of input image [channel, height, width]\n",
    "        filters: python list of integers, defining the number of filters in the Conv layers\n",
    "        kernels: python list of integers, defining kernel height and width\n",
    "        strides: python list of integers, defining stride for each Conv layer\n",
    "        n_out: number of output classes\n",
    "\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Sequential(                  # input shape (3, 64, 64)\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=image_shape[0],  # input channels = 3\n",
    "                        out_channels=filters[0],     # number of output filters = 8\n",
    "                        kernel_size=kernels[0],      # kernel size = 3\n",
    "                        stride=strides[0],           # filter movement/step = 2\n",
    "                        padding=1,                   # if want same width and length of this image after con2d\n",
    "                        ),                           # output shape (8, 32, 32)\n",
    "                    nn.ReLU(),)                      # activation\n",
    "\n",
    "        self.conv2 = nn.Sequential(                 # input shape (8, 32, 32)\n",
    "                    nn.Conv2d(filters[0], filters[1],\n",
    "                              kernels[1], strides[1],\n",
    "                              padding=1),           # output shape (16, 16, 16)\n",
    "                    nn.ReLU(),)                     # activation\n",
    "\n",
    "        self.out = nn.Linear(16 * 16 * 16, n_out)   # fully connected layer, output 6 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 16 * 16 * 16)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model by passing hyper-parameters to CNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare hyper-parameters\n",
    "input_shape = train_set_x.shape\n",
    "filters = [8, 16]\n",
    "kernels = [3, 3]\n",
    "strides = [2, 2]\n",
    "n_out = int(Y_test.max().item()) + 1\n",
    "\n",
    "# Define model\n",
    "model = CNN(input_shape[1:], filters, kernels, strides, n_out)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print(model)\n",
    "# Expected output: CNN(conv1: Sequential(0:..., 1:...),\n",
    "#                      conv2: Sequential(0:..., 1:...), out: Linear...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "Now let's define loss function to train our model. The `nn` package also contains definitions of many popular [loss functions](https://pytorch.org/docs/master/nn.html#loss-functions). In this case we will use Cross-entropy (CE) as our loss function.\n",
    "\n",
    "**Optimizer**\n",
    "\n",
    "We will update model parameters automatically using `Adam` optimizer. You can check which other optimizers are available in the [documentation](https://pytorch.org/docs/master/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define training loop**\n",
    "\n",
    "Now everything is ready to define our training loop using `model`, `loss` and `optimizer` created above. We don't need to pass those as parameters of train function, since they are declared as part of PyTorch computational Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, X_test, Y_test, minibatch_size=64, num_epochs=20, seed=3):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    train_x -- numpy.array, training input,\n",
    "    train_y -- numpy.array, training output target,\n",
    "    X_test -- Tensor, test input,\n",
    "    Y_test -- Tensor, test output target,\n",
    "    minibatch_size -- int, number of training samples per batch,\n",
    "    num_epochs -- int, number of training epochs,\n",
    "    seed -- int, seed of random number\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(train_x, train_y, minibatch_size, seed)\n",
    "\n",
    "        for step, (minibatch_x, minibatch_y) in enumerate(minibatches):\n",
    "            x = torch.tensor(minibatch_x, device=device, dtype=dtype, requires_grad=False)\n",
    "            y = torch.tensor(minibatch_y, device=device, dtype=torch.long)\n",
    "            output = model(x)               # cnn output\n",
    "            loss = loss_func(output, y)     # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "\n",
    "        print('Epoch: ', epoch + 1, '| train loss: %.4f' % loss.data.cpu().numpy())\n",
    "\n",
    "    test_output = model(X_test)\n",
    "    train_output = model(torch.tensor(train_x, dtype=dtype, device=device))\n",
    "    pred_test_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "    pred_train_y = torch.max(train_output, 1)[1].data.squeeze()\n",
    "    accuracy_test = float(sum(pred_test_y == Y_test)) / float(Y_test.size(0)) * 100\n",
    "    accuracy_train = float(sum(pred_train_y.cpu().numpy() == train_y)) / float(train_y.shape[0]) * 100\n",
    "    print('\\nTrain accuracy: %.1f\\n' % accuracy_train, 'Test accuracy: %.1f' % accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_set_x, train_set_y, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
