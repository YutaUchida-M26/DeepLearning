{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Create MLP using NumPy #\n",
    "\n",
    "The aim of this exercise is to create simple neural network using NumPy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import all libraries, that are needed to finish this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need define some helper functions to load data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"Read train and test datasets\"\"\"\n",
    "    train_dataset = h5py.File('/datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('/datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use a simple image dataset that has two classes *cat* and *non-cat*. Every image is represented with numpy array of shape \\[num_pixels, num_pixels, 3\\] (3 is number of RGB channels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/datasets/train_catvnoncat.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5cc065833876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the data (cat/non-cat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Figuring out the dimensions and shapes of the problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a898359d1148>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Read train and test datasets\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/datasets/train_catvnoncat.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_set_x_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# your train set features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_set_y_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# your train set labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Python3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Python3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/datasets/train_catvnoncat.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "\n",
    "# Figuring out the dimensions and shapes of the problem\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print(\"Number of training examples: m_train = \" + str(m_train))\n",
    "print(\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print(\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print(\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print(\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print(\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see images from dataset by changing index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-887a0cb40e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example of a picture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m print(\"y = \" + str(train_set_y[:, index]) + \", it's a '\" +\n\u001b[1;32m      5\u001b[0m       classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \"' picture.\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 30\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print(\"y = \" + str(train_set_y[:, index]) + \", it's a '\" +\n",
    "      classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know from the lecture, standard neural network expects 1D vector as input. So, we need to reshape these images in a numpy-array of shape (num_pixels $*$ num_pixels $*$ 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a9a5d24a83bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reshape the training and test examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "print(\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print(\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print(\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print(\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print(\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x_flatten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cbe22dbf0aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# \"Standardize\" the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_set_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set_x_flatten\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_set_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_x_flatten\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x_flatten' is not defined"
     ]
    }
   ],
   "source": [
    "# \"Standardize\" the data\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building basic parts of Neural Network ##\n",
    "\n",
    "**Mathematical expression of the neural network classifier with one linear hidden layer**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The general methodology to build a Neural Network is to:**\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Training loop:\n",
    "    - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "We will start with building helper functions to compute steps 1-3 and then merge them into one function we call `model()`. Once we've built `model()` and learnt the right parameters, we can make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Defining structure of the neural network ###\n",
    "\n",
    "We will use shapes of input and output to define sizes of input and output layers and set size of hidden layer to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]  # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]  # size of output layer\n",
    "\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 10\n",
      "The size of the hidden layer is: n_h = 4\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "# Let's check if we implemented previous function correctly\n",
    "X_tmp = np.zeros([10, 50])\n",
    "Y_tmp = np.zeros([1, 50])\n",
    "n_x, n_h, n_y = layer_sizes(X_tmp, Y_tmp)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))  # Expected output: 10\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))  # Expected output: 4\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))  # Expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Initialize parameters of the model ###\n",
    "\n",
    "Given sizes of input, hidden and output layer we can create and initialize:\n",
    "- weight matrices with random values.\n",
    "- biases with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(2)   # we set up a seed so that your output matches ours\n",
    "                        # although the initialization is random.\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-4.16757847e-03 -5.62668272e-04 -2.13619610e-02  1.64027081e-02\n",
      "  -1.79343559e-02 -8.41747366e-03  5.02881417e-03 -1.24528809e-02\n",
      "  -1.05795222e-02 -9.09007615e-03]\n",
      " [ 5.51454045e-03  2.29220801e-02  4.15393930e-04 -1.11792545e-02\n",
      "   5.39058321e-03 -5.96159700e-03 -1.91304965e-04  1.17500122e-02\n",
      "  -7.47870949e-03  9.02525097e-05]\n",
      " [-8.78107893e-03 -1.56434170e-03  2.56570452e-03 -9.88779049e-03\n",
      "  -3.38821966e-03 -2.36184031e-03 -6.37655012e-03 -1.18761229e-02\n",
      "  -1.42121723e-02 -1.53495196e-03]\n",
      " [-2.69056960e-03  2.23136679e-02 -2.43476758e-02  1.12726505e-03\n",
      "   3.70444537e-03  1.35963386e-02  5.01857207e-03 -8.44213704e-03\n",
      "   9.76147160e-08  5.42352572e-03]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.00313508  0.00771012 -0.01868091  0.01731185]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))  # [[-4.16757847e-03, ...], ... ,[... , 5.42352572e-03]]\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))  # [[0.], [0.], [0.], [0.]]\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))  # [[-0.00313508  0.00771012 -0.01868091  0.01731185]]\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))  # [[0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training loop ###\n",
    "\n",
    "First we need to implemente `sigmoid()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-x))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "s = sigmoid(0)\n",
    "print('sigmoid(0) = ' + str(s))  # Expected output: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.1:**\n",
    "    Implement forward propagation function\n",
    "    \n",
    "- Look above at the mathematical representation of our classifier.\n",
    "- We will use the function `np.tanh()` for hidden layer and function `sigmoid()` for output layer.\n",
    "- The steps we need to implement are:\n",
    "1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "\n",
    "Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1:\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.add(np.dot(W1, X), b1)\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.add(np.dot(W2, A1), b2)\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.5\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "A2, cache = forward_propagation(X_tmp, parameters)\n",
    "\n",
    "print(np.mean(cache['Z1']), np.mean(cache['A1']), np.mean(cache['Z2']), np.mean(cache['A2']))\n",
    "# Expected output: 0.0 0.0 0.0 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2:** Implement `compute_cost()` to compute the value of the cost $J$.\n",
    "\n",
    "Now that we have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, we can compute the cost function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{5}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (5)\n",
    "\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "\n",
    "    Returns:\n",
    "    cost -- binary cross-entropy cost given equation (5)\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]  # number of examples\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(-np.log(A2), Y) + np.multiply(-np.log(1 - A2), 1 - Y)\n",
    "    cost = (1./m) * np.nansum(logprobs)\n",
    "\n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.\n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_tmp, parameters)))\n",
    "# Expected output: 0.69314718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.3:** Implement backward_propagation function\n",
    "\n",
    "Using the cache computed during forward propagation, we can now implement `backward_propagation()`.\n",
    "\n",
    "Backpropagation is usually the hardest (most mathematical) part in deep learning. Here are equations in the notation we used in this notebook.  \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = a^{[2](i)} - y^{(i)}\\tag{6}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{1}{m}  \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T}\\tag{7} $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\frac{1}{m}  \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}\\tag{8}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2})\\tag{9} $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{1}{m}  \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T \\tag{10} $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\frac{1}{m}  \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}\\tag{11}$$\n",
    "\n",
    "- Note that $*$ denotes elementwise multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2.\n",
    "    dZ2 = A2 - Y  # equation (6)\n",
    "    dW2 = (1./m) * (np.dot(dZ2, A1.T))  # equation (7)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)  # equation (8)\n",
    "    dZ1 = (W2.T * dZ2) * (1 - np.power(A1, 2))  # equation (9)\n",
    "    dW1 = (1./m) * np.dot(dZ1, X.T)  # equation (10)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)  # equation (11)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "db1 = [[-0.00156754]\n",
      " [ 0.00385506]\n",
      " [-0.00934045]\n",
      " [ 0.00865592]]\n",
      "dW2 = [[0. 0. 0. 0.]]\n",
      "db2 = [[0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "grads = backward_propagation(parameters, cache, X_tmp, Y_tmp)\n",
    "print(\"dW1 = \" + str(grads[\"dW1\"]))  # Expected output: [[0., 0., ...], ...,[..., 0., 0.]]\n",
    "print(\"db1 = \" + str(grads[\"db1\"]))  # Expected output: [[-0.00156754], [ 0.00385506], [-0.00934045], [ 0.00865592]]\n",
    "print(\"dW2 = \" + str(grads[\"dW2\"]))  # Expected output: [[0. 0. 0. 0.]]\n",
    "print(\"db2 = \" + str(grads[\"db2\"]))  # Expected output: [[0.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.4:** Implement the update rule. \n",
    "\n",
    "We will use gradient descent as the update rule. We have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2). We can retrieve those from dictionaries `parameters` and `grads`.\n",
    "\n",
    "**General gradient descent rule**: \n",
    "$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.4\n",
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-4.16757847e-03 -5.62668272e-04 -2.13619610e-02  1.64027081e-02\n",
      "  -1.79343559e-02 -8.41747366e-03  5.02881417e-03 -1.24528809e-02\n",
      "  -1.05795222e-02 -9.09007615e-03]\n",
      " [ 5.51454045e-03  2.29220801e-02  4.15393930e-04 -1.11792545e-02\n",
      "   5.39058321e-03 -5.96159700e-03 -1.91304965e-04  1.17500122e-02\n",
      "  -7.47870949e-03  9.02525097e-05]\n",
      " [-8.78107893e-03 -1.56434170e-03  2.56570452e-03 -9.88779049e-03\n",
      "  -3.38821966e-03 -2.36184031e-03 -6.37655012e-03 -1.18761229e-02\n",
      "  -1.42121723e-02 -1.53495196e-03]\n",
      " [-2.69056960e-03  2.23136679e-02 -2.43476758e-02  1.12726505e-03\n",
      "   3.70444537e-03  1.35963386e-02  5.01857207e-03 -8.44213704e-03\n",
      "   9.76147160e-08  5.42352572e-03]]\n",
      "b1 = [[ 0.00188105]\n",
      " [-0.00462607]\n",
      " [ 0.01120854]\n",
      " [-0.01038711]]\n",
      "W2 = [[-0.00313508  0.00771012 -0.01868091  0.01731185]]\n",
      "b2 = [[-0.6]]\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))  # Expected output: [[-4.16757847e-03, ...], ... ,[...,5.42352572e-03]]\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))  # Expected output: [[ 0.00188105], [-0.00462607], [ 0.01120854], [-0.01038711]]\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))  # Expected output: [[-0.00313508  0.00771012 -0.01868091  0.01731185]]\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))  # Expected output: [[-0.6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions ###\n",
    "\n",
    "Now we can calculate prediction probability of example to belong to class cat.\n",
    "\n",
    "We can use parameters from the `forward_propagation()` to get those probabilities. By comparing them with threshold we can get predicted class (non-cat: 0 / cat: 1).\n",
    "\n",
    "$$ predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (non-cat: 0 / cat: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2 > 0.5\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate Step 1, Step 2 and Step 3 and build full model###\n",
    "\n",
    "Now we can build our neural network model in `model()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, n_h, num_iterations=10000,\n",
    "          learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- number of iterations in gradient descent loop\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 200 iterations\n",
    "\n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X_train, Y_train)[0]\n",
    "    n_y = layer_sizes(X_train, Y_train)[2]\n",
    "    costs = []\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X_train, parameters)\n",
    "\n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y_train, parameters)\n",
    "\n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X_train, Y_train)\n",
    "\n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate) \n",
    "\n",
    "        # Print the cost every 200 iterations\n",
    "        if i % 200 == 0:\n",
    "            if print_cost:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(parameters, X_test)\n",
    "    Y_prediction_train = predict(parameters, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\": Y_prediction_train,\n",
    "         \"costs\": costs,\n",
    "         \"parameters\": parameters,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0cfa6f116fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m d = model(train_set_x, train_set_y, test_set_x, test_set_y, 10,\n\u001b[0m\u001b[1;32m      2\u001b[0m           num_iterations=2000, learning_rate=0.05, print_cost=True)\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Expected output: Cost after iteration 0:  0.693045 ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x' is not defined"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, 10,\n",
    "          num_iterations=2000, learning_rate=0.05, print_cost=True)\n",
    "# Expected output: Cost after iteration 0:  0.693045 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of learning rate ###\n",
    "\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "\n",
    "- Compare the learning curve of your model with several choices of learning rates. Try your own values.\n",
    "\n",
    "**Optional**\n",
    "\n",
    "- Find optimal learning rate and hidden layer size for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f2c36842fa50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning rate is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, 10,\n\u001b[0m\u001b[1;32m      6\u001b[0m                            num_iterations=2000, learning_rate=i)\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-------------------------------------------------------\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print(\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, 10,\n",
    "                           num_iterations=2000, learning_rate=i)\n",
    "    print('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label=str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create MLP using PyTorch Tensor and Autograd modules. #\n",
    "\n",
    "The aim of this part is to create similar structure as in part 1 using PyTorch library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define default data type and device for Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)  # we set up a seed so that your output matches ours although the initialization is random.\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already load our data in the part 1, but here we need to reshape it in different order. Now we will reshape these images in a numpy-array of shape (num_samples, num_pixels  $*$ num_pixels $*$  3) and labels in a numpy array of shape (num_samples, num_labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-66de897893d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reshape the training and test examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1)\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1)\n",
    "\n",
    "# Reshape labels dataset\n",
    "train_set_y = train_set_y.T\n",
    "test_set_y = test_set_y.T\n",
    "\n",
    "# \"Standardize\" the data\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create PyTorch Tensors to use it further to train and test our PyTorch model.\n",
    "\n",
    "**Hint**\n",
    "Here we use previously defined `device` and `dtype`. And we set `requires_grad` parameter to False since it's only data holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fe07d28d5298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(train_set_x, dtype=dtype, requires_grad=False)\n",
    "Y_train = torch.tensor(train_set_y, dtype=dtype, requires_grad=False)\n",
    "X_test = torch.tensor(test_set_x, dtype=dtype, requires_grad=False)\n",
    "Y_test = torch.tensor(test_set_y, dtype=dtype, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of exercise we will follow the same metodology as in **part 1**.\n",
    "\n",
    "**The general methodology to build a Neural Network is to:**\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Training loop:\n",
    "    - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Defining structure of the neural network ###\n",
    "\n",
    "First, we implement function `layer_sizes()`. We will use sizes of input and output Tensors to define sizes of input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (input size, number of examples)\n",
    "    Y -- labels tensor of shape (output size, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.size(1)\n",
    "    n_y = Y.size(1)\n",
    "\n",
    "    return n_x, n_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 10\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "# Let's check implemention\n",
    "X_tmp = torch.zeros([50, 10])\n",
    "Y_tmp = torch.zeros([50, 1])\n",
    "n_x, n_y = layer_sizes(X_tmp, Y_tmp)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))  # Expected output: 10\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))  # Expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Initialize parameters of the model ###\n",
    "Given sizes of input, hidden and output layer we can create and initialize:\n",
    "- weight matrices with random values.\n",
    "- biases with zeros.\n",
    "\n",
    "**Hint** Here we set `requires_grad` parameter to `True`. This will allow us to compute gradients automatically later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_x, n_h)\n",
    "                    b1 -- bias vector of shape (1, n_h)\n",
    "                    W2 -- weight matrix of shape (n_h, n_y)\n",
    "                    b2 -- bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create empty tensors\n",
    "    W1 = torch.empty(n_x, n_h, dtype=dtype, requires_grad=True)\n",
    "    b1 = torch.empty((1, n_h), dtype=dtype, requires_grad=True)\n",
    "    W2 = torch.empty(n_h, n_y, dtype=dtype, requires_grad=True)\n",
    "    b2 = torch.zeros((1, n_y), dtype=dtype, requires_grad=True)\n",
    "\n",
    "    # Initialize tensors\n",
    "    torch.nn.init.orthogonal_(W1)\n",
    "    torch.nn.init.constant_(b1, 0)\n",
    "    torch.nn.init.orthogonal_(W2)\n",
    "    torch.nn.init.constant_(b2, 0)\n",
    "\n",
    "    assert (W1.size() == (n_x, n_h))\n",
    "    assert (b1.size() == (1, n_h))\n",
    "    assert (W2.size() == (n_h, n_y))\n",
    "    assert (b2.size() == (1, n_y))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = tensor([[-0.3628,  0.1662, -0.2899, -0.6219, -0.2913],\n",
      "        [ 0.4070, -0.1957, -0.0847, -0.2243,  0.1036],\n",
      "        [-0.1991, -0.1431,  0.2902, -0.2499, -0.4159],\n",
      "        [-0.1508, -0.5302, -0.0923,  0.0286,  0.2523],\n",
      "        [ 0.0104, -0.0122,  0.2748,  0.3865,  0.0074],\n",
      "        [-0.1738, -0.0189,  0.7404,  0.0265, -0.2077],\n",
      "        [ 0.1099,  0.5941, -0.2199,  0.3713, -0.3163],\n",
      "        [ 0.6446,  0.2359,  0.3195, -0.4244,  0.0486],\n",
      "        [ 0.3299, -0.4473, -0.1918,  0.1768, -0.7212],\n",
      "        [ 0.2714, -0.1526, -0.0720, -0.0174,  0.0463]], requires_grad=True)\n",
      "b1 = tensor([[0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "W2 = tensor([[-0.3817],\n",
      "        [ 0.7263],\n",
      "        [-0.3035],\n",
      "        [ 0.3854],\n",
      "        [ 0.2936]], requires_grad=True)\n",
      "b2 = tensor([[0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "parameters = initialize_parameters(n_x, 5, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))  # Expected output: tensor([[-0.3628,...],...,[...,0.0463]])\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))  # Expected output: tensor([[ 0.,  0.,  0.,  0.,  0.]])\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))  # Expected output: tensor([[-0.3817],[ 0.7263],[-0.3035],[ 0.3854],[ 0.2936]])\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))  # Expected output: tensor([[ 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training loop ###\n",
    "**Step 3.1:**\n",
    "    Implement forward propagation function\n",
    "    \n",
    "- Look above at the mathematical representation of our classifier.\n",
    "- We will use the function `torch.tanh()` for hidden layer and function `torch.sigmoid()` for output layer.\n",
    "- The steps we need to implement are:\n",
    "1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "\n",
    "When we implemented this step in **part 1** we stored values needed in the backpropagation in \"`cache`\". Here, the forward pass of our network will define a computational graph. So nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Later we will use [autograd](https://pytorch.org/docs/master/autograd.html) package to calculate backward pass. Since all functions are stored in the computational graph, there's no need to keep them in separate dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1:\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (m, n_x)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    # Replace None with your own code\n",
    "    Z1 = torch.add(torch.mm(X, W1), b1)\n",
    "    A1 = torch.tanh(Z1)\n",
    "    Z2 = torch.add(torch.mm(A1, W2), b2)\n",
    "    A2 = torch.sigmoid(Z2)\n",
    "\n",
    "    assert(A2.size() == (X.size(0), 1))\n",
    "\n",
    "    return A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "\n",
    "A2 = forward_propagation(X_tmp, parameters)\n",
    "\n",
    "print(torch.mean(A2).item())\n",
    "# Expected output: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2:** Implement `compute_cost()` to manually compute the value of the cost $J$. \n",
    "\n",
    "Now that we have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, we can compute the cost function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{5}$$\n",
    "\n",
    "We will implement it in the same way as in **part1**, but now we will use PyTorch functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (5)\n",
    "\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (number of examples, 1)\n",
    "    Y -- \"true\" labels vector of shape (number of examples, 1)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "\n",
    "    Returns:\n",
    "    cost -- binary cross-entropy cost given equation (5)\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.size(0)  # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = (-torch.log(A2) * Y) + (-torch.log(1 - A2) * (1 - Y))\n",
    "    cost = (1./m) * torch.sum(logprobs) \n",
    "\n",
    "    cost = torch.squeeze(cost)     # makes sure cost is the dimension we expect.\n",
    "                                   # E.g., turns [[17]] into 17 \n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6931471228599548\n"
     ]
    }
   ],
   "source": [
    "# Let's check implementation\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_tmp, parameters).item()))\n",
    "# Expected output: 0.693147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.3:** Backward propagation\n",
    "\n",
    "Here we will use automatic differentiation to automate the computation of backward passes in neural networks. This call will compute the gradient of loss with respect to all Tensors with requires_grad=True. So we will call it later directly in the training loop.\n",
    "\n",
    "The `autograd` package in PyTorch provides exactly this functionality. When using `autograd`, the forward pass of your network will define a computational graph. Backpropagating through this graph then allows you to easily compute gradients. See documentation for more details: https://pytorch.org/docs/master/autograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.4:** \n",
    "Implement the update rule using gradient descent. \n",
    "\n",
    "**General gradient descent rule**: \n",
    "\n",
    "$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "**Hint** \n",
    "We can access gradients using `parameter.grad` that is automatically computed in the backward pass.\n",
    "After updating parameters, computed gradients should be zeroed. We will use `.zero_()` for in-place operation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Since we use manual update of parameters, we need to wrap in torch.no_grad()\n",
    "    # because all parameters have requires_grad=True, but we don't need to track this.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Update each parameter\n",
    "        parameters[\"W1\"] -= learning_rate * parameters[\"W1\"].grad\n",
    "        parameters[\"b1\"] -= learning_rate * parameters[\"b1\"].grad\n",
    "        parameters[\"W2\"] -= learning_rate * parameters[\"W2\"].grad\n",
    "        parameters[\"b2\"] -= learning_rate * parameters[\"b2\"].grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        parameters[\"W1\"].grad.zero_()\n",
    "        parameters[\"b1\"].grad.zero_()\n",
    "        parameters[\"W2\"].grad.zero_()\n",
    "        parameters[\"b2\"].grad.zero_()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Now we can calculate prediction probability of example to belong to class cat.\n",
    "\n",
    "We can use parameters from the `forward_propagation()` to get those probabilities. By comparing them with threshold we can get predicted class (non-cat: 0 / cat: 1).\n",
    "\n",
    "$$ predictions = y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}  $$\n",
    "\n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```. Remember that it will return `LongTensor`, so we need to cast it back to `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, n_x)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (non-cat: 0 / cat: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2 = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5).float()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate Step 1, Step 2 and Step 3 and build full model###\n",
    "\n",
    "Now we can build our neural network model in `model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, n_h, num_iterations=10000,\n",
    "          learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- number of iterations in gradient descent loop\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- if True, print the cost every 200 iterations\n",
    "\n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "\n",
    "    n_x, n_y = layer_sizes(X_train, Y_train)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2 = forward_propagation(X_train, parameters)\n",
    "\n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y_train, parameters)\n",
    "\n",
    "        # Backpropagation. Compute gradient of the cost function with respect to all the \n",
    "        # learnable parameters of the model. Use autograd to compute the backward pass.\n",
    "        cost.backward()\n",
    "\n",
    "        # Gradient descent parameter update.\n",
    "        parameters = update_parameters(parameters, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost.item()))\n",
    "\n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict2(parameters, X_test)\n",
    "    Y_prediction_train = predict2(parameters, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - torch.mean(torch.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - torch.mean(torch.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\": Y_prediction_train,\n",
    "         \"parameters\": parameters,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(X_train, Y_train, X_test, Y_test, 15,\n",
    "          num_iterations=2000, learning_rate=0.05, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional ##\n",
    "\n",
    "- Try different initializations in `initialize_parameters()` and find most suitable one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
