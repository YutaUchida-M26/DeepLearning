{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4. #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Manually create RNN with PyTorch #\n",
    "\n",
    "The aim of this exercise is to create manually a simple RNN. We will train our RNN to learn sine function. During training we will be feeding our model with one data point at a time, that is why we need only one input neuron x1, and we want to predict the value at next time step. Our input sequence x consists of 20 data points, and the target sequence is the same as the input sequence but it ‘s shifted by one-time step into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import all libraries, that are needed to finish this exercise and do some housekeeping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Set the random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Get PyTorch float type\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation ##\n",
    "\n",
    "Now we will generate the training data, where x is an input sequence and y is a target sequence. For this purpose, we create the function `DataGen`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGen(seq_length):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    seq_length - Length of the sine wave (in points)\n",
    "\n",
    "    Returns:\n",
    "    x - single input sequence (tensor of size [20, 1])\n",
    "    y - single target sequence (tensor of size [20, 1])\n",
    "    data_time_steps - sequence of sine wave time steps (np_array of size [21,])\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate time sequence of seq_length + 1 steps\n",
    "    data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "\n",
    "    # Generate sine wave\n",
    "    data = np.sin(data_time_steps)\n",
    "\n",
    "    # Reshape as one column vector\n",
    "    data.resize((seq_length + 1, 1))\n",
    "\n",
    "    # Make input tensor\n",
    "    x = torch.tensor(data[:-1], dtype=dtype, requires_grad=False)\n",
    "\n",
    "    # Make target tensor (shifted input by one sample)\n",
    "    y = torch.tensor(data[1:], dtype=dtype, requires_grad=False)\n",
    "\n",
    "    return x, y, data_time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RNN model ##\n",
    "\n",
    "Next, we will create our network manually without using the PyTorch nn module. We need to create two weight matrices, w1 of size (input_size, hidden_size) for input to hidden connections, and a w2 matrix of size (hidden_size, output_size) for hidden to output connection. Weights are initialized using a normal distribution with zero mean. Note that the input of the hidden nodes consists of the input data (of size 1) and outputs of the hidden nodes from the previous time step!\n",
    "\n",
    "We define forward method, it takes input data, previous hidden state vector as arguments and uses two weights matrices. We will create input vector by concatenating the data with the previous hidden state vector. We perform dot product between the input vector and weight matrix W1, then apply `tanh` function as nonlinearity, which works better with RNNs than sigmoid. Then we perform another dot product between new hidden state vector and weight matrix W2. We want to predict continuous value, so we do not apply any nonlinearity at this stage.\n",
    "\n",
    "Note that hidden state vector will be used to populate context neurons at the next time step. That is why we return  hidden state vector along with the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "\n",
    "        # Set correct input layer size.\n",
    "        input_size = data_size + hidden_size\n",
    "\n",
    "        # Remember hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Create weight matrices. We dont use biases.\n",
    "        self.W1 = torch.empty(input_size, hidden_size, dtype=dtype, requires_grad=True)\n",
    "        self.W2 = torch.empty(hidden_size, output_size, dtype=dtype, requires_grad=True)\n",
    "\n",
    "        # Initialize weight matrices with normaly distributed values: mean = 0, std = 0.4 or 0.3.\n",
    "        torch.nn.init.normal_(self.W1, 0.0, 0.4)\n",
    "        torch.nn.init.normal_(self.W2, 0.0, 0.3)\n",
    "\n",
    "    def forward(self, data, previous_hidden):\n",
    "\n",
    "        # Concatenate input data with the output from the hidden layer of the previous time step\n",
    "        input = torch.cat((data, previous_hidden), 1)\n",
    "        # Calculate hidden state vector\n",
    "        hidden = torch.tanh(input.mm(self.W1))\n",
    "        # Calculate output\n",
    "        output = hidden.mm(self.W2)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def UpdateParams(self, lr):\n",
    "\n",
    "        # Update models parameters - W1 and W2 using learning rate 'lr'\n",
    "        with torch.no_grad():\n",
    "            self.W1 -= lr * self.W1.grad\n",
    "            self.W2 -= lr * self.W2.grad\n",
    "\n",
    "            # Clear the gradients\n",
    "            self.W1.grad.zero_()\n",
    "            self.W2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also manually define the loss function. Since the RNN outputs are continuous, we use Mean Squared Error (MSE) criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    pred - predicted value (RNN output)\n",
    "    target - taget value\n",
    "\n",
    "    Returns squared difference between 'pred' and 'target' values\n",
    "    \"\"\"\n",
    "\n",
    "    return (pred - target).pow(2).sum()/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ##\n",
    "\n",
    "Our training loop will be structured as follows.\n",
    "\n",
    "- The outer loop iterates over each epoch. Epoch is defined as one pass of all training data. At the beginning of each epoch, we need to initialize our hidden state vector with zeros.\n",
    "\n",
    "- The inner loop runs through each element of the sequence. We run forward method to perform forward pass which returns prediction and previous_hidden state which will be used for next time step. Then we compute Mean Square Error (MSE),  which is a natural choice when we want to predict continuous values.  By running backward() method on the loss we calculating gradients, then we update the weights. We’re supposed to clear the gradients at each iteration by calling zero_() method otherwise gradients will be accumulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, epochs, lr):\n",
    "\n",
    "    # Get the timesteps number\n",
    "    timesteps = x.size(0)\n",
    "\n",
    "    # Trainig loop\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Set total loss to zero\n",
    "        loss = 0\n",
    "\n",
    "        # Initialize the hidden state vector\n",
    "        previous_hidden = torch.zeros((1, model.hidden_size), dtype=dtype, requires_grad=True)\n",
    "\n",
    "        # Loop over each timestep\n",
    "        for j in range(timesteps):\n",
    "\n",
    "            # Get input and target for the current timestep\n",
    "            input = x[j:(j+1)]\n",
    "            target = y[j:(j+1)]\n",
    "\n",
    "            # Forward operation\n",
    "            pred, previous_hidden = model.forward(input, previous_hidden)\n",
    "\n",
    "            # Summ losses for each timestep\n",
    "            loss += loss_fn(pred, target)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\"Epoch: {} loss {}\".format(i, loss.data.numpy()))\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        model.UpdateParams(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train the RNN. First, we define the RNN layer sizes. Then, the number of training epochs, input sequence length and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experimental conditions\n",
    "input_size, hidden_size, output_size = 1, 6, 1\n",
    "epochs = 300\n",
    "seq_length = 20\n",
    "lr = 0.1\n",
    "\n",
    "# Generate input sequence and terget sequence\n",
    "x, y, time_steps = DataGen(seq_length)\n",
    "\n",
    "# Create the RNN\n",
    "rnn = Model(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train\n",
    "train(rnn, x, y, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions ###\n",
    "\n",
    "Once our model is trained, we can make predictions, at each step of the sequence we will feed the model with single data point and ask the model to predict one value at the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "\n",
    "    # Initialize list of predicted values.\n",
    "    predictions = []\n",
    "\n",
    "    # Initialize the hidden state vector\n",
    "    previous_hidden = torch.zeros((1, model.hidden_size), dtype=dtype, requires_grad=False)\n",
    "\n",
    "    # Number of timesteps\n",
    "    timesteps = x.size(0)\n",
    "\n",
    "    # Loop over each input point\n",
    "    for i in range(timesteps):\n",
    "\n",
    "        # Get current input value\n",
    "        input = x[i:(i+1)]\n",
    "\n",
    "        # Make Prediction\n",
    "        pred, previous_hidden = model.forward(input, previous_hidden)\n",
    "\n",
    "        # Save predictions\n",
    "        predictions.append(pred.data.numpy().ravel()[0])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RNN prediction\n",
    "pred = predict(rnn, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the true and predicted points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input and predicted points\n",
    "plt.scatter(time_steps[:-1], x.data.numpy(), s=90, label=\"Actual\")\n",
    "plt.scatter(time_steps[1:], pred, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model did a pretty good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a sine wave with RNN ###\n",
    "\n",
    "In the prediction mode, we fed the RNN with the actual input values. Now, we will generate a sine wave by giving the RNN only the first value which we generate randomly. Then, the predicted value will be the next RNN input and so on. This way the RNN can generate a sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate(model, input, timesteps=20):\n",
    "\n",
    "    # Initialize list of generated values\n",
    "    generated = []\n",
    "\n",
    "    # Initialize the hidden state vector\n",
    "    previous_hidden = torch.zeros((1, model.hidden_size), dtype=dtype, requires_grad=False)\n",
    "\n",
    "    # Set the first input point\n",
    "    pred = input\n",
    "\n",
    "    # Loop over the desired sequence length\n",
    "    for i in range(timesteps):\n",
    "\n",
    "        # Predict value and use it as the next input point\n",
    "        pred, previous_hidden = model.forward(pred, previous_hidden)\n",
    "\n",
    "        # Save generated points\n",
    "        generated.append(pred.data.numpy().ravel()[0])\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run RNN in generation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make random first input\n",
    "inp = torch.rand((1, 1), dtype=dtype, requires_grad=False)\n",
    "\n",
    "# Generate sine wave\n",
    "gen = Generate(rnn, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the generated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generated points\n",
    "plt.scatter(time_steps[1:], gen, label=\"Generated\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the generated wave is very close to a sine wave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create RNN based name classification system #\n",
    "\n",
    "We will be building and training a basic character-level RNN to classify words. A character-level RNN reads words as a series of characters - outputting a prediction and \"hidden state\" at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data ###\n",
    "\n",
    "Data are located in **/datasets/data.zip** and will be extracted into the current directory.\n",
    "\n",
    "Included in the data/names directory are 18 text files named as \"[Language].txt\". Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language, {language: [names ...]}. The generic variables \"category\" and \"line\" (for language and name in our case) are used for later extensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all necessary data\n",
    "def load_dataset():\n",
    "    # Get data (extract the zip file if not done already)\n",
    "    if not os.path.exists('data'):\n",
    "        zip_file = zipfile.ZipFile('/datasets/data.zip')\n",
    "        zip_file.extractall()\n",
    "        zip_file.close()\n",
    "        print('Extracted data files in ./data folder')\n",
    "    else:\n",
    "        print('Data in ./data folder already available.')\n",
    "\n",
    "    # Get a list of all possible letters\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    # The number of all possible letters\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "    # Turn a Unicode string to plain ASCII\n",
    "    def unicodeToAscii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in all_letters)\n",
    "\n",
    "    # Dictionary to hold language categories and the names of each category\n",
    "    category_lines = {}\n",
    "    # List of all languages (categories)\n",
    "    all_categories = []\n",
    "\n",
    "    # Read a file and split into lines\n",
    "    def readLines(filename):\n",
    "        lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "    for filename in glob.glob('data/names/*.txt'):\n",
    "        category = filename.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(filename)\n",
    "        category_lines[category] = lines\n",
    "\n",
    "    n_categories = len(all_categories)\n",
    "\n",
    "    # Retruns:\n",
    "    # 1. all_letters    - list of all possible letters\n",
    "    # 2. n_letters      - the number of all letters\n",
    "    # 3. category_lines - data dictionary\n",
    "    # 4. all_categories - list of all categories\n",
    "    # 5. n_categories   - number of all categories\n",
    "    return all_letters, n_letters, category_lines, all_categories, n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "all_letters, n_letters, category_lines, all_categories, n_categories = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have category_lines, a dictionary mapping each category (language) to a list of lines (names). We also kept track of all_categories (just a list of languages) and n_categories for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at letters, categories and several Italian names.\n",
    "print(all_letters)\n",
    "print(all_categories)\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Names into Tensors ###\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size <1 x n_letters>. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. \"b\" = <0 1 0 0 0 ...>.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix <line_length x 1 x n_letters>.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in batches - we're just using a batch size of 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Network ###\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved cloning the parameters of a layer over several timesteps. The layers held hidden state and gradients which are now entirely handled by the graph itself. This means you can implement a RNN in a very \"pure\" way, as regular feed-forward layers.\n",
    "\n",
    "This RNN module is just 2 linear layers which operate on an input and hidden state, with a LogSoftmax layer after the output.\n",
    "\n",
    "<img src=\"https://i.imgur.com/Z2xbySO.png\" alt=\"\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN\n",
    "class RNN(nn.Module):\n",
    "    # Initialize the RNN with each layer size\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Remenber the hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Create two layers: input-to-hidden and input-to-output\n",
    "        # Note that both the layers take the data and (previous) hidden output as input\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # since we do classification we use SoftMax as output layer activation\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Forward processing\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        # Combine the input data and the output from the previous hidden layer\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        # Calculate current hidden output\n",
    "        hidden = self.i2h(combined)\n",
    "        # Calculate current output\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    # Initialize the hidden layer\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the RNN with a hidden layer of 64 neurons/cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the Tensor for the current letter) and a previous hidden state (which we initialize as zeros at first). We'll get back the output (probability of each language) and a next hidden state (which we keep for the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the Tensor for the current letter) and a previous hidden state (which we initialize as zeros at first). We'll get back the output (probability of each language) and a next hidden state (which we keep for the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a <1 x n_categories> Tensor, where every item is the likelihood of that category (higher is more likely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ##\n",
    "\n",
    "### Preparing for Training ###\n",
    "\n",
    "Before going into training we should make a few helper functions. The first is to interpret the output of the network, which we know to be a likelihood of each category. We can use Tensor.topk to get the index of the greatest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns random element of list 'lst'\n",
    "def randomChoice(lst):\n",
    "    return lst[random.randint(0, len(lst) - 1)]\n",
    "\n",
    "\n",
    "# Returns random training sample (name:language pair) in normal and tensor formats\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "\n",
    "# Check some random pairs\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network ###\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples, have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function nn.NLLLoss is appropriate, since the last layer of the RNN is nn.LogSoftmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    " - Create input and target tensors\n",
    " - Create a zeroed initial hidden state\n",
    " - Read each letter in and\n",
    "   - Keep hidden state for next letter\n",
    " - Compare final output to target\n",
    " - Back-propagate\n",
    " - Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005  # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "\n",
    "    # Zero the hidden state\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    # Remove gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # Loop over the input sequence\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_f(output, category_tensor)\n",
    "\n",
    "    # Calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update RNN parametrs\n",
    "    with torch.no_grad():\n",
    "        for param in rnn.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "    # Return the RNN output and loss\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the train function returns both the output and loss we can print its guesses and also keep track of loss for plotting. Since there are 1000s of examples we print only every print_every examples, and take an average of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 20000\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), current_loss / print_every, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Results ###\n",
    "Plotting the historical loss from all_losses shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Results ###\n",
    "\n",
    "To see how well the network performs on different categories, we will create a confusion matrix, indicating for every actual language (rows) which language the network guesses (columns). To calculate the confusion matrix a bunch of samples are run through the network with evaluate(), which is the same as train() minus the backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "# Number of tests\n",
    "n_confusion = 2000\n",
    "\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "numticks = len(all_categories)\n",
    "ax.set_xticks(list(range(numticks)))\n",
    "ax.set_yticks(list(range(numticks)))\n",
    "ax.set_xticklabels(all_categories, rotation=90)\n",
    "ax.set_yticklabels(all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
